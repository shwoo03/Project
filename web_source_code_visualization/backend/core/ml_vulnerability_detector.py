"""
ML-based Vulnerability Detector.

This module implements machine learning models for:
1. Vulnerability classification
2. False positive filtering
3. Severity prediction
4. Confidence scoring

Models:
- Random Forest / XGBoost for classification
- Ensemble methods for false positive reduction
- Rule-based fallback for known patterns
"""

from typing import List, Dict, Set, Optional, Any, Tuple
from dataclasses import dataclass, field
from enum import Enum
import json
import os
import pickle
import logging
from pathlib import Path
import numpy as np
from collections import defaultdict

from .ml_feature_extractor import (
    MLFeatureExtractor, VulnerabilityFeatures, CodeContext, 
    get_feature_extractor
)


# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class VulnerabilityClass(Enum):
    """Classification of vulnerability status."""
    TRUE_POSITIVE = "true_positive"
    FALSE_POSITIVE = "false_positive"
    LIKELY_TRUE = "likely_true"       # High confidence true positive
    LIKELY_FALSE = "likely_false"     # High confidence false positive
    UNCERTAIN = "uncertain"           # Needs human review


class Severity(Enum):
    """Vulnerability severity levels."""
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"
    INFO = "info"


@dataclass
class PredictionResult:
    """Result of ML-based vulnerability prediction."""
    feature_id: str
    vulnerability_type: str
    
    # Classification
    is_vulnerability: bool
    classification: VulnerabilityClass
    confidence: float  # 0.0 - 1.0
    
    # Severity
    predicted_severity: Severity
    severity_confidence: float
    
    # Explanation
    contributing_factors: List[str] = field(default_factory=list)
    risk_factors: List[str] = field(default_factory=list)
    mitigating_factors: List[str] = field(default_factory=list)
    
    # Recommendations
    recommendations: List[str] = field(default_factory=list)
    
    # Original data
    features: Optional[VulnerabilityFeatures] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            "feature_id": self.feature_id,
            "vulnerability_type": self.vulnerability_type,
            "is_vulnerability": self.is_vulnerability,
            "classification": self.classification.value,
            "confidence": round(self.confidence, 3),
            "predicted_severity": self.predicted_severity.value,
            "severity_confidence": round(self.severity_confidence, 3),
            "contributing_factors": self.contributing_factors,
            "risk_factors": self.risk_factors,
            "mitigating_factors": self.mitigating_factors,
            "recommendations": self.recommendations,
        }


class RuleBasedClassifier:
    """
    Rule-based classifier for known vulnerability patterns.
    Used as baseline and for high-confidence predictions.
    """
    
    # High confidence true positive patterns
    HIGH_CONFIDENCE_VULN_RULES = {
        "sql_injection": {
            "patterns": [
                ("known_vuln_pattern", 0.8),
                ("string_concat_used", 0.3),
                ("format_string_used", 0.3),
            ],
            "required_score": 0.7,
        },
        "command_injection": {
            "patterns": [
                ("known_vuln_pattern", 0.9),
                ("direct_user_input", 0.3),
                ("uses_framework_input", 0.2),
            ],
            "required_score": 0.6,
        },
        "xss": {
            "patterns": [
                ("known_vuln_pattern", 0.8),
                ("dangerous_pattern_count", 0.4),
                ("direct_user_input", 0.2),
            ],
            "required_score": 0.6,
        },
    }
    
    # High confidence false positive patterns
    FALSE_POSITIVE_RULES = {
        "sanitizer_present": 0.8,
        "sanitizer_pattern_count": 0.6,
        "validated_input": 0.4,
        "framework_protection": 0.5,
        "is_typed": 0.2,
    }
    
    def predict(self, features: VulnerabilityFeatures) -> Tuple[float, List[str]]:
        """
        Rule-based prediction.
        
        Returns:
            Tuple of (vulnerability_score, explanation_list)
        """
        vuln_type = features.vulnerability_type.lower().replace("-", "_").replace(" ", "_")
        
        vuln_score = 0.0
        fp_score = 0.0
        explanations = []
        
        # Check vulnerability patterns
        if vuln_type in self.HIGH_CONFIDENCE_VULN_RULES:
            rules = self.HIGH_CONFIDENCE_VULN_RULES[vuln_type]
            for pattern_name, weight in rules["patterns"]:
                value = features.pattern.get(pattern_name, 0.0)
                if value > 0.5:
                    vuln_score += weight * value
                    explanations.append(f"Dangerous pattern: {pattern_name}")
        
        # Check false positive indicators
        for pattern_name, weight in self.FALSE_POSITIVE_RULES.items():
            value = (
                features.pattern.get(pattern_name, 0.0) or
                features.contextual.get(pattern_name, 0.0) or
                features.semantic.get(pattern_name, 0.0)
            )
            if value > 0.5:
                fp_score += weight * value
                explanations.append(f"Mitigating factor: {pattern_name}")
        
        # Combine scores
        final_score = max(0.0, min(1.0, vuln_score - fp_score * 0.7))
        
        return final_score, explanations


class EnsembleModel:
    """
    Ensemble model combining multiple approaches:
    1. Rule-based classifier
    2. Feature-weighted classifier
    3. Pattern matching confidence
    """
    
    def __init__(self):
        self.rule_classifier = RuleBasedClassifier()
        self.feature_weights = self._initialize_weights()
        self.threshold = 0.5
    
    def _initialize_weights(self) -> Dict[str, float]:
        """Initialize feature importance weights."""
        return {
            # High importance (strong vulnerability indicators)
            "known_vuln_pattern": 2.5,
            "dangerous_pattern_count": 2.0,
            "direct_user_input": 1.8,
            "string_concat_used": 1.5,
            "format_string_used": 1.5,
            
            # Medium importance
            "uses_framework_input": 1.2,
            "crosses_function_boundary": 1.0,
            "function_complexity": 0.8,
            "distance_source_sink": -0.3,  # Longer distance = less likely
            
            # Mitigating factors (negative weight)
            "sanitizer_present": -2.5,
            "sanitizer_pattern_count": -2.0,
            "validated_input": -1.5,
            "framework_protection": -1.2,
            "is_typed": -0.5,
            "has_type_annotation": -0.4,
        }
    
    def predict(self, features: VulnerabilityFeatures) -> Tuple[float, float, List[str]]:
        """
        Ensemble prediction.
        
        Returns:
            Tuple of (vulnerability_probability, confidence, explanations)
        """
        # Rule-based score
        rule_score, rule_explanations = self.rule_classifier.predict(features)
        
        # Feature-weighted score
        weighted_score = 0.0
        feature_explanations = []
        
        all_features = {
            **features.structural,
            **features.semantic,
            **features.contextual,
            **features.pattern,
        }
        
        for feature_name, weight in self.feature_weights.items():
            value = all_features.get(feature_name, 0.0)
            contribution = value * weight
            weighted_score += contribution
            
            if abs(contribution) > 0.3:
                direction = "increases" if contribution > 0 else "decreases"
                feature_explanations.append(
                    f"{feature_name} {direction} vulnerability likelihood"
                )
        
        # Normalize weighted score to 0-1
        weighted_score = 1.0 / (1.0 + np.exp(-weighted_score))  # Sigmoid
        
        # Combine scores (weighted average)
        combined_score = 0.6 * weighted_score + 0.4 * rule_score
        
        # Calculate confidence based on score extremity
        confidence = abs(combined_score - 0.5) * 2  # 0 at 0.5, 1 at extremes
        confidence = min(0.95, confidence + 0.3)  # Add baseline confidence
        
        all_explanations = rule_explanations + feature_explanations
        
        return combined_score, confidence, all_explanations


class MLVulnerabilityDetector:
    """
    Main ML-based vulnerability detector.
    
    Combines feature extraction, ensemble classification, and severity prediction
    to provide accurate vulnerability detection with confidence scores.
    """
    
    # Severity weights based on vulnerability type
    SEVERITY_WEIGHTS = {
        "code_injection": Severity.CRITICAL,
        "command_injection": Severity.CRITICAL,
        "sql_injection": Severity.HIGH,
        "xss": Severity.HIGH,
        "path_traversal": Severity.HIGH,
        "ssrf": Severity.HIGH,
        "template_injection": Severity.HIGH,
        "open_redirect": Severity.MEDIUM,
        "information_disclosure": Severity.MEDIUM,
        "general": Severity.MEDIUM,
    }
    
    # Confidence thresholds
    HIGH_CONFIDENCE_THRESHOLD = 0.85
    MEDIUM_CONFIDENCE_THRESHOLD = 0.65
    LOW_CONFIDENCE_THRESHOLD = 0.4
    
    def __init__(self, model_path: Optional[str] = None):
        self.feature_extractor = get_feature_extractor()
        self.ensemble_model = EnsembleModel()
        self.model_path = model_path
        
        # Statistics tracking
        self.stats = {
            "total_analyzed": 0,
            "true_positives": 0,
            "false_positives": 0,
            "uncertain": 0,
            "by_type": defaultdict(int),
            "by_severity": defaultdict(int),
        }
        
        # Load trained model if available
        self._load_model()
    
    def _load_model(self):
        """Load pre-trained model if available."""
        if self.model_path and os.path.exists(self.model_path):
            try:
                with open(self.model_path, 'rb') as f:
                    model_data = pickle.load(f)
                    self.ensemble_model.feature_weights = model_data.get(
                        'feature_weights', 
                        self.ensemble_model.feature_weights
                    )
                    self.ensemble_model.threshold = model_data.get('threshold', 0.5)
                    logger.info(f"Loaded ML model from {self.model_path}")
            except Exception as e:
                logger.warning(f"Failed to load model: {e}")
    
    def save_model(self, path: str):
        """Save current model state."""
        model_data = {
            'feature_weights': self.ensemble_model.feature_weights,
            'threshold': self.ensemble_model.threshold,
            'stats': dict(self.stats),
        }
        with open(path, 'wb') as f:
            pickle.dump(model_data, f)
        logger.info(f"Saved ML model to {path}")
    
    def analyze(
        self,
        taint_flow: Dict[str, Any],
        code_content: str,
        context: Optional[CodeContext] = None,
        symbol_table: Optional[Dict] = None,
        call_graph: Optional[Dict] = None
    ) -> PredictionResult:
        """
        Analyze a single potential vulnerability.
        
        Args:
            taint_flow: Taint flow data from taint analyzer
            code_content: Source code content
            context: Optional code context
            symbol_table: Optional symbol table
            call_graph: Optional call graph
            
        Returns:
            PredictionResult with classification and confidence
        """
        # Create context if not provided
        if context is None:
            context = CodeContext(
                file_path=taint_flow.get("file_path", "unknown"),
                function_name=taint_flow.get("function_name"),
                line_start=taint_flow.get("source_line", 0),
                line_end=taint_flow.get("sink_line", 0),
            )
        
        # Extract features
        features = self.feature_extractor.extract_features(
            taint_flow, code_content, context, symbol_table, call_graph
        )
        
        # Get ensemble prediction
        vuln_prob, confidence, explanations = self.ensemble_model.predict(features)
        
        # Determine classification
        classification = self._classify(vuln_prob, confidence)
        is_vulnerability = vuln_prob >= self.ensemble_model.threshold
        
        # Predict severity
        severity, severity_conf = self._predict_severity(
            features, vuln_prob, classification
        )
        
        # Generate risk and mitigating factors
        risk_factors, mitigating_factors = self._analyze_factors(features)
        
        # Generate recommendations
        recommendations = self._generate_recommendations(
            features, classification, severity
        )
        
        # Update statistics
        self._update_stats(classification, features.vulnerability_type, severity)
        
        return PredictionResult(
            feature_id=features.feature_id,
            vulnerability_type=features.vulnerability_type,
            is_vulnerability=is_vulnerability,
            classification=classification,
            confidence=confidence,
            predicted_severity=severity,
            severity_confidence=severity_conf,
            contributing_factors=explanations,
            risk_factors=risk_factors,
            mitigating_factors=mitigating_factors,
            recommendations=recommendations,
            features=features,
        )
    
    def batch_analyze(
        self,
        taint_flows: List[Dict[str, Any]],
        code_contents: Dict[str, str],
        filter_false_positives: bool = True
    ) -> List[PredictionResult]:
        """
        Analyze multiple potential vulnerabilities.
        
        Args:
            taint_flows: List of taint flow data
            code_contents: Map of file paths to code content
            filter_false_positives: Whether to filter likely false positives
            
        Returns:
            List of PredictionResults
        """
        results = []
        
        for flow in taint_flows:
            file_path = flow.get("file_path", "")
            code = code_contents.get(file_path, "")
            
            result = self.analyze(flow, code)
            
            # Optionally filter false positives
            if filter_false_positives:
                if result.classification in [
                    VulnerabilityClass.FALSE_POSITIVE,
                    VulnerabilityClass.LIKELY_FALSE
                ]:
                    continue
            
            results.append(result)
        
        # Sort by confidence and severity
        results.sort(
            key=lambda x: (
                x.predicted_severity != Severity.CRITICAL,
                x.predicted_severity != Severity.HIGH,
                -x.confidence
            )
        )
        
        return results
    
    def _classify(
        self, 
        probability: float, 
        confidence: float
    ) -> VulnerabilityClass:
        """Classify based on probability and confidence."""
        if confidence >= self.HIGH_CONFIDENCE_THRESHOLD:
            if probability >= 0.7:
                return VulnerabilityClass.TRUE_POSITIVE
            elif probability <= 0.3:
                return VulnerabilityClass.FALSE_POSITIVE
        
        if confidence >= self.MEDIUM_CONFIDENCE_THRESHOLD:
            if probability >= 0.6:
                return VulnerabilityClass.LIKELY_TRUE
            elif probability <= 0.4:
                return VulnerabilityClass.LIKELY_FALSE
        
        return VulnerabilityClass.UNCERTAIN
    
    def _predict_severity(
        self,
        features: VulnerabilityFeatures,
        vuln_prob: float,
        classification: VulnerabilityClass
    ) -> Tuple[Severity, float]:
        """Predict vulnerability severity."""
        vuln_type = features.vulnerability_type.lower().replace("-", "_").replace(" ", "_")
        
        # Base severity from type
        base_severity = self.SEVERITY_WEIGHTS.get(vuln_type, Severity.MEDIUM)
        
        # Adjust based on features
        severity_boost = 0
        
        # Critical indicators
        if features.pattern.get("known_vuln_pattern", 0) > 0.5:
            severity_boost += 1
        if features.pattern.get("direct_user_input", 0) > 0.5:
            severity_boost += 1
        if features.contextual.get("is_entry_point", 0) > 0.5:
            severity_boost += 1
        
        # Severity reduction
        if features.contextual.get("sanitizer_present", 0) > 0.5:
            severity_boost -= 1
        if features.pattern.get("validated_input", 0) > 0.5:
            severity_boost -= 1
        
        # Map to severity levels
        severity_order = [Severity.INFO, Severity.LOW, Severity.MEDIUM, 
                        Severity.HIGH, Severity.CRITICAL]
        base_index = severity_order.index(base_severity)
        final_index = max(0, min(4, base_index + severity_boost))
        final_severity = severity_order[final_index]
        
        # Confidence based on classification confidence
        if classification in [VulnerabilityClass.TRUE_POSITIVE, 
                             VulnerabilityClass.FALSE_POSITIVE]:
            severity_conf = 0.9
        elif classification in [VulnerabilityClass.LIKELY_TRUE, 
                               VulnerabilityClass.LIKELY_FALSE]:
            severity_conf = 0.7
        else:
            severity_conf = 0.5
        
        return final_severity, severity_conf
    
    def _analyze_factors(
        self, 
        features: VulnerabilityFeatures
    ) -> Tuple[List[str], List[str]]:
        """Extract risk and mitigating factors."""
        risk_factors = []
        mitigating_factors = []
        
        # Risk factors
        if features.pattern.get("known_vuln_pattern", 0) > 0.5:
            risk_factors.append("Known vulnerability pattern detected")
        if features.pattern.get("direct_user_input", 0) > 0.5:
            risk_factors.append("Direct user input used without validation")
        if features.pattern.get("string_concat_used", 0) > 0.5:
            risk_factors.append("String concatenation used for queries/commands")
        if features.contextual.get("is_entry_point", 0) > 0.5:
            risk_factors.append("Located in entry point (directly accessible)")
        if features.semantic.get("crosses_function_boundary", 0) > 0.5:
            risk_factors.append("Tainted data crosses function boundaries")
        
        # Mitigating factors
        if features.contextual.get("sanitizer_present", 0) > 0.5:
            mitigating_factors.append("Sanitizer detected in data flow")
        if features.pattern.get("validated_input", 0) > 0.5:
            mitigating_factors.append("Input validation present")
        if features.pattern.get("framework_protection", 0) > 0.5:
            mitigating_factors.append("Framework security features enabled")
        if features.semantic.get("is_typed", 0) > 0.5:
            mitigating_factors.append("Type annotations present")
        if features.pattern.get("sanitizer_pattern_count", 0) > 0.3:
            mitigating_factors.append("Sanitization patterns found in code")
        
        return risk_factors, mitigating_factors
    
    def _generate_recommendations(
        self,
        features: VulnerabilityFeatures,
        classification: VulnerabilityClass,
        severity: Severity
    ) -> List[str]:
        """Generate fix recommendations."""
        recommendations = []
        vuln_type = features.vulnerability_type.lower()
        
        if classification == VulnerabilityClass.UNCERTAIN:
            recommendations.append("Manual review recommended due to uncertain classification")
        
        # Type-specific recommendations
        if "sql" in vuln_type:
            recommendations.extend([
                "Use parameterized queries or prepared statements",
                "Implement an ORM (Object-Relational Mapping)",
                "Validate and sanitize all user inputs",
            ])
        elif "xss" in vuln_type:
            recommendations.extend([
                "Encode output for the appropriate context (HTML, JavaScript, URL)",
                "Use framework's built-in escaping functions",
                "Implement Content Security Policy (CSP) headers",
            ])
        elif "command" in vuln_type or "code" in vuln_type:
            recommendations.extend([
                "Avoid executing shell commands with user input",
                "Use safe APIs like subprocess with shell=False",
                "Implement strict input validation with allowlists",
            ])
        elif "path" in vuln_type:
            recommendations.extend([
                "Use os.path.basename() to extract filename only",
                "Validate paths against a whitelist of allowed directories",
                "Use os.path.realpath() to resolve symlinks",
            ])
        elif "ssrf" in vuln_type:
            recommendations.extend([
                "Validate and whitelist allowed URLs/hosts",
                "Disable unnecessary URL schemes (file://, gopher://)",
                "Use a URL parser to extract and validate hostname",
            ])
        
        # Severity-based recommendations
        if severity in [Severity.CRITICAL, Severity.HIGH]:
            recommendations.insert(0, "⚠️ High priority - Address immediately")
        
        return recommendations
    
    def _update_stats(
        self,
        classification: VulnerabilityClass,
        vuln_type: str,
        severity: Severity
    ):
        """Update internal statistics."""
        self.stats["total_analyzed"] += 1
        
        if classification in [VulnerabilityClass.TRUE_POSITIVE, 
                             VulnerabilityClass.LIKELY_TRUE]:
            self.stats["true_positives"] += 1
        elif classification in [VulnerabilityClass.FALSE_POSITIVE, 
                               VulnerabilityClass.LIKELY_FALSE]:
            self.stats["false_positives"] += 1
        else:
            self.stats["uncertain"] += 1
        
        self.stats["by_type"][vuln_type] += 1
        self.stats["by_severity"][severity.value] += 1
    
    def get_statistics(self) -> Dict[str, Any]:
        """Get analysis statistics."""
        total = self.stats["total_analyzed"]
        if total == 0:
            return self.stats
        
        return {
            **self.stats,
            "false_positive_rate": round(
                self.stats["false_positives"] / total * 100, 2
            ),
            "true_positive_rate": round(
                self.stats["true_positives"] / total * 100, 2
            ),
            "uncertainty_rate": round(
                self.stats["uncertain"] / total * 100, 2
            ),
        }
    
    def reset_statistics(self):
        """Reset statistics tracking."""
        self.stats = {
            "total_analyzed": 0,
            "true_positives": 0,
            "false_positives": 0,
            "uncertain": 0,
            "by_type": defaultdict(int),
            "by_severity": defaultdict(int),
        }
    
    def update_weights(self, feedback: Dict[str, bool]):
        """
        Update model weights based on feedback.
        
        Args:
            feedback: Dict mapping feature_id to is_true_positive
        """
        # This would be used for online learning
        # For now, just log the feedback
        logger.info(f"Received feedback for {len(feedback)} predictions")
        # Future: Implement gradient-based weight updates


# Singleton instance
_ml_detector: Optional[MLVulnerabilityDetector] = None

def get_ml_detector() -> MLVulnerabilityDetector:
    """Get singleton MLVulnerabilityDetector instance."""
    global _ml_detector
    if _ml_detector is None:
        _ml_detector = MLVulnerabilityDetector()
    return _ml_detector


def analyze_with_ml(
    taint_flows: List[Dict[str, Any]],
    code_contents: Dict[str, str],
    filter_false_positives: bool = True
) -> Dict[str, Any]:
    """
    Convenience function to analyze vulnerabilities with ML.
    
    Args:
        taint_flows: List of taint flow data
        code_contents: Map of file paths to code content
        filter_false_positives: Whether to filter likely false positives
        
    Returns:
        Dict with results and statistics
    """
    detector = get_ml_detector()
    results = detector.batch_analyze(
        taint_flows, 
        code_contents, 
        filter_false_positives
    )
    
    return {
        "results": [r.to_dict() for r in results],
        "statistics": detector.get_statistics(),
        "filtered_count": len(taint_flows) - len(results) if filter_false_positives else 0,
    }
