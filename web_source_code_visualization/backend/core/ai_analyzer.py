
import os
import groq
import traceback
from typing import Dict, Any
from dotenv import load_dotenv

class AIAnalyzer:
    def __init__(self):
        # Force load .env from project root (3 levels up from backend/core/ai_analyzer.py)
        # backend/core/ai_analyzer.py -> backend/core -> backend -> root
        root_env = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')
        load_dotenv(root_env)
        
        self.api_key = os.getenv("GROQ_API_KEY")
        if not self.api_key:
            print(f"Warning: GROQ_API_KEY is not set. Tried loading from: {root_env}")
            self.client = None
        else:
            print(f"[AI] GROQ_API_KEY loaded successfully from {root_env}")
            self.client = groq.Groq(api_key=self.api_key)

        # Priority List - ÏïàÏ†ïÏ†ÅÏù∏ Î™®Îç∏ Ïö∞ÏÑ†
        self.models = [
            "llama-3.3-70b-versatile",    # 1. Í∞ÄÏû• ÏïàÏ†ïÏ†Å
            "llama-3.1-8b-instant",       # 2. Îπ†Î•∏ fallback
            "qwen/qwen3-32b",             # 3. ÎåÄÏïà
        ]

    def analyze_code(self, code: str, context: str = "", referenced_files: Dict[str, str] = None) -> Dict[str, Any]:
        if not self.client:
            return {
                "error": "GROQ_API_KEY is missing. Please set it in .env file.",
                "analysis": "AI Analysis is disabled."
            }
            
        system_prompt = (
            "ÎãπÏã†ÏùÄ CTF/Wargame Î¨∏Ï†ú ÌíÄÏù¥ Î∞è Ïõπ Ìï¥ÌÇπ Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§. "
            "Ï£ºÏñ¥ÏßÑ ÏΩîÎìúÏôÄ ÌîÑÎ°úÏ†ùÌä∏ Ï†ÑÏ≤¥ Îß•ÎùΩ(Context)ÏùÑ Î∂ÑÏÑùÌïòÏó¨ Î≥¥Ïïà Ï∑®ÏïΩÏ†êÏùÑ Ï∞æÏïÑÎÇ¥ÏÑ∏Ïöî. "
            "Ïù¥ ÏΩîÎìúÎäî ÏõåÍ≤åÏûÑ(Wargame) Î¨∏Ï†úÏùò ÏùºÎ∂ÄÏù¥ÎØÄÎ°ú, **Í≥µÍ≤© ÏãúÎÇòÎ¶¨Ïò§ÏôÄ Flag ÌöçÎìù Î∞©Î≤ï**ÏóêÎßå ÏßëÏ§ëÌï¥Ïïº Ìï©ÎãàÎã§.\n\n"
            "**ÏùëÎãµ Í∞ÄÏù¥ÎìúÎùºÏù∏ (Î∞òÎìúÏãú Ï§ÄÏàò):**\n"
            "1. **Markdown Ìè¨Îß∑ Ï†ÅÏö©**: Notion Ïä§ÌÉÄÏùºÏùò ÍπîÎÅîÌïú MarkdownÏùÑ ÏÇ¨Ïö©ÌïúÎã§.\n"
            "2. **Íµ¨Ï°∞ÌôîÎêú Ìó§Îçî**: ÎåÄÏ£ºÏ†úÎäî `#`, Ï§ëÏ£ºÏ†úÎäî `##`, ÏÜåÏ£ºÏ†úÎäî `###`ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Í≥ÑÏ∏µ Íµ¨Ï°∞Î•º Î™ÖÌôïÌûà ÌïúÎã§.\n"
            "3. **Î¨∏Ï≤¥ ÌÜµÏùº**: Î™®Îì† Î¨∏Ïû•ÏùÄ Î∞òÎìúÏãú '**~Îã§.**', '**~Ïù¥Îã§.**', '**~ÌïòÎã§.**', '**~ÏûàÎã§.**' Îì±Ïùò ÌèâÏÑúÎ¨∏ÏúºÎ°ú ÎÅùÎß∫ÎäîÎã§.\n"
            "4. **ÎÇ¥Ïö© Íµ¨ÏÑ±**:\n"
            "   - `# ÏÉÅÌÉú`: '‚úÖ **ÏïàÏ†ÑÌï®**' ÎòêÎäî 'üö® **Ï∑®ÏïΩÌï®**' ÌëúÏãú.\n"
            "   - `# ÌïµÏã¨ Ï∑®ÏïΩÏ†ê`: Î∞úÍ≤¨Îêú Ï∑®ÏïΩÏ†ê Î™ÖÏπ≠ (Ïòà: Reflected XSS, Cookie Injection).\n"
            "   - `# Í≥µÍ≤© Î∂ÑÏÑù`: Ï∑®ÏïΩÏ†ê Î∞úÏÉù ÏõêÏù∏Í≥º ÏïÖÏö© Î°úÏßÅÏùÑ ÎÖºÎ¶¨Ï†ÅÏúºÎ°ú ÏÑúÏà†ÌïúÎã§.\n"
            "   - `# PoC (Proof of Concept)`: Í≥µÍ≤© ÌéòÏù¥Î°úÎìú, Î™ÖÎ†πÏñ¥, Í≥µÍ≤© ÏàúÏÑú Îì±ÏùÑ ÏΩîÎìú Î∏îÎ°ùÍ≥º Ìï®Íªò ÏÉÅÏÑ∏Ìûà ÏûëÏÑ±ÌïúÎã§.\n\n"
            "**Ï£ºÏùòÏÇ¨Ìï≠:**\n"
            "- 'ÎåÄÏùë Î∞©Ïïà'Ïù¥ÎÇò 'Î≥¥Ïïà Í∞ÄÏù¥Îìú'Îäî **Ï†àÎåÄ Ìè¨Ìï®ÌïòÏßÄ ÏïäÎäîÎã§**.\n"
            "- Î∂àÌïÑÏöîÌïú ÏÑúÎ°†Ïù¥ÎÇò Ïù∏ÏÇ¨ÎßêÏùÄ ÏÉùÎûµÌïúÎã§.\n"
            "- Î∞òÎìúÏãú **ÌïúÍµ≠Ïñ¥(Korean)**Î°ú ÏûëÏÑ±ÌïúÎã§."
        )
        
        # Build Reference Context
        ref_context = ""
        if referenced_files:
            ref_context = "\n\n**[Ï∞∏Í≥† ÌååÏùº Î∞è Ìï®Ïàò Ï†ïÏùò]**:\n"
            for fpath, fcontent in referenced_files.items():
                ref_context += f"--- {fpath} ---\n{fcontent}\n\n"

        user_prompt = f"Code to analyze:\n```\n{code}\n```\n\nContext:\n{context}\n{ref_context}"
        
        # ÌîÑÎ°¨ÌîÑÌä∏ Í∏∏Ïù¥ Ï≤¥ÌÅ¨ (ÌÜ†ÌÅ∞ Ï†úÌïú Î∞©ÏßÄ)
        total_chars = len(system_prompt) + len(user_prompt)
        print(f"[AI] Total prompt length: {total_chars} chars (~{total_chars // 4} tokens)")
        
        if total_chars > 30000:  # ÏïΩ 7500 ÌÜ†ÌÅ∞ Ïù¥ÏÉÅÏù¥Î©¥ Í≤ΩÍ≥†
            print(f"[AI] Warning: Prompt is very long, may hit token limits")

        errors_log = []
        for model in self.models:
            try:
                print(f"[AI] Attempting analysis with model: {model}")
                chat_completion = self.client.chat.completions.create(
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    model=model,
                    temperature=0.1,
                    max_tokens=2048,
                    timeout=60,  # 60Ï¥à ÌÉÄÏûÑÏïÑÏõÉ
                )
                
                analysis = chat_completion.choices[0].message.content
                print(f"[AI] Success with model: {model}")
                return {
                    "model": model,
                    "analysis": analysis,
                    "success": True
                }

            except groq.RateLimitError as e:
                error_msg = f"Rate limit for {model}: {str(e)[:100]}"
                print(f"[AI] {error_msg}")
                errors_log.append(error_msg)
                continue
            except groq.NotFoundError as e:
                error_msg = f"Model {model} not found: {str(e)[:100]}"
                print(f"[AI] {error_msg}")
                errors_log.append(error_msg)
                continue
            except groq.APIConnectionError as e:
                error_msg = f"Connection error for {model}: {str(e)[:100]}"
                print(f"[AI] {error_msg}")
                errors_log.append(error_msg)
                continue
            except groq.APITimeoutError as e:
                error_msg = f"Timeout for {model}: {str(e)[:100]}"
                print(f"[AI] {error_msg}")
                errors_log.append(error_msg)
                continue
            except Exception as e:
                error_msg = f"Error with {model}: {type(e).__name__}: {str(e)[:150]}"
                print(f"[AI] {error_msg}")
                print(f"[AI] Traceback: {traceback.format_exc()}")
                errors_log.append(error_msg)
                continue

        return {
            "error": "All AI models failed or rate limited.",
            "analysis": f"Î∂ÑÏÑù Ïã§Ìå®. ÏÉÅÏÑ∏ Ïò§Î•ò:\n" + "\n".join(errors_log) if errors_log else "Ïïå Ïàò ÏóÜÎäî Ïò§Î•ò",
            "success": False
        }
