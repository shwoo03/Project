import os
import groq
from typing import Dict, Any
from dotenv import load_dotenv

class AIAnalyzer:
    def __init__(self):
        # Force load .env from project root (3 levels up from backend/core/ai_analyzer.py)
        # backend/core/ai_analyzer.py -> backend/core -> backend -> root
        root_env = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), '.env')
        load_dotenv(root_env)
        
        self.api_key = os.getenv("GROQ_API_KEY")
        if not self.api_key:
            print(f"Warning: GROQ_API_KEY is not set. Tried loading from: {root_env}")
            self.client = None
        else:
            self.client = groq.Groq(api_key=self.api_key)

        # Priority List (User Defined)
        self.models = [
            "openai/gpt-oss-120b",        # 1. 1st Priority
            "llama-3.3-70b-versatile",    # 2. 2nd Priority
            "qwen/qwen3-32b",             # 3. 3rd Priority
            "llama-3.1-8b-instant"        # 4. Ultimate Fallback (added for safety)
        ]

    def analyze_code(self, code: str, context: str = "") -> Dict[str, Any]:
        if not self.client:
            return {
                "error": "GROQ_API_KEY is missing. Please set it in .env file.",
                "analysis": "AI Analysis is disabled."
            }

        system_prompt = (
            "ë‹¹ì‹ ì€ CTF/Wargame ë¬¸ì œ í’€ì´ ë° ì›¹ í•´í‚¹ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
            "ì£¼ì–´ì§„ ì½”ë“œì™€ í”„ë¡œì íŠ¸ ì „ì²´ ë§¥ë½(Context)ì„ ë¶„ì„í•˜ì—¬ ë³´ì•ˆ ì·¨ì•½ì ì„ ì°¾ì•„ë‚´ì„¸ìš”. "
            "ì´ ì½”ë“œëŠ” ì›Œê²Œì„(Wargame) ë¬¸ì œì˜ ì¼ë¶€ì´ë¯€ë¡œ, **ê³µê²© ì‹œë‚˜ë¦¬ì˜¤ì™€ Flag íšë“ ë°©ë²•**ì—ë§Œ ì§‘ì¤‘í•´ì•¼ í•©ë‹ˆë‹¤.\n\n"
            "**ì‘ë‹µ ê°€ì´ë“œë¼ì¸ (ë°˜ë“œì‹œ ì¤€ìˆ˜):**\n"
            "1. **Markdown í¬ë§· ì ìš©**: Notion ìŠ¤íƒ€ì¼ì˜ ê¹”ë”í•œ Markdownì„ ì‚¬ìš©í•œë‹¤.\n"
            "2. **êµ¬ì¡°í™”ëœ í—¤ë”**: ëŒ€ì£¼ì œëŠ” `#`, ì¤‘ì£¼ì œëŠ” `##`, ì†Œì£¼ì œëŠ” `###`ì„ ì‚¬ìš©í•˜ì—¬ ê³„ì¸µ êµ¬ì¡°ë¥¼ ëª…í™•íˆ í•œë‹¤.\n"
            "3. **ë¬¸ì²´ í†µì¼**: ëª¨ë“  ë¬¸ì¥ì€ ë°˜ë“œì‹œ '**~ë‹¤.**', '**~ì´ë‹¤.**', '**~í•˜ë‹¤.**', '**~ìˆë‹¤.**' ë“±ì˜ í‰ì„œë¬¸ìœ¼ë¡œ ëë§ºëŠ”ë‹¤.\n"
            "4. **ë‚´ìš© êµ¬ì„±**:\n"
            "   - `# ìƒíƒœ`: 'âœ… **ì•ˆì „í•¨**' ë˜ëŠ” 'ğŸš¨ **ì·¨ì•½í•¨**' í‘œì‹œ.\n"
            "   - `# í•µì‹¬ ì·¨ì•½ì `: ë°œê²¬ëœ ì·¨ì•½ì  ëª…ì¹­ (ì˜ˆ: Reflected XSS, Cookie Injection).\n"
            "   - `# ê³µê²© ë¶„ì„`: ì·¨ì•½ì  ë°œìƒ ì›ì¸ê³¼ ì•…ìš© ë¡œì§ì„ ë…¼ë¦¬ì ìœ¼ë¡œ ì„œìˆ í•œë‹¤.\n"
            "   - `# PoC (Proof of Concept)`: ê³µê²© í˜ì´ë¡œë“œ, ëª…ë ¹ì–´, ê³µê²© ìˆœì„œ ë“±ì„ ì½”ë“œ ë¸”ë¡ê³¼ í•¨ê»˜ ìƒì„¸íˆ ì‘ì„±í•œë‹¤.\n\n"
            "**ì£¼ì˜ì‚¬í•­:**\n"
            "- 'ëŒ€ì‘ ë°©ì•ˆ'ì´ë‚˜ 'ë³´ì•ˆ ê°€ì´ë“œ'ëŠ” **ì ˆëŒ€ í¬í•¨í•˜ì§€ ì•ŠëŠ”ë‹¤**.\n"
            "- ë¶ˆí•„ìš”í•œ ì„œë¡ ì´ë‚˜ ì¸ì‚¬ë§ì€ ìƒëµí•œë‹¤.\n"
            "- ë°˜ë“œì‹œ **í•œêµ­ì–´(Korean)**ë¡œ ì‘ì„±í•œë‹¤."
        )

        user_prompt = f"Code to analyze:\n```\n{code}\n```"

        for model in self.models:
            try:
                print(f"Attempting analysis with model: {model}")
                chat_completion = self.client.chat.completions.create(
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    model=model,
                    temperature=0.1,
                    max_tokens=2048,
                )
                
                analysis = chat_completion.choices[0].message.content
                return {
                    "model": model,
                    "analysis": analysis,
                    "success": True
                }

            except groq.RateLimitError as e:
                print(f"Rate limit exceeded for {model}. Falling back...")
                continue
            except groq.NotFoundError as e:
                print(f"Model {model} not found or deprecated. Falling back...")
                continue
            except Exception as e:
                print(f"Error with model {model}: {e}")
                # For other errors, we might want to try next model or fail?
                # Let's try next model just in case it's a specific model outage
                continue

        return {
            "error": "All AI models failed or rate limited.",
            "analysis": "Could not complete analysis due to high traffic.",
            "success": False
        }

