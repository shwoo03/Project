import os
import requests
from bs4 import BeautifulSoup
import pymongo
import datetime
import time
import logging
from dotenv import load_dotenv

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

def get_env_var():
    """
    .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
    """
    current_dir = os.path.dirname(os.path.abspath(__file__))
    env_path = os.path.join(current_dir, ".env")
    
    if os.path.exists(env_path):
        load_dotenv(env_path)
    else:
        load_dotenv()

    mongo_uri = os.getenv("MONGO_URI")
    webhook_url = os.getenv("DISCORD_WEBHOOK")

    if not mongo_uri or not webhook_url:
        logger.error("MONGO_URI or DISCORD_WEBHOOK not found.")
        return None

    return {
        "MONGO_URI": mongo_uri,
        "DISCORD_WEBHOOK": webhook_url
    }

from playwright.sync_api import sync_playwright

def parse_jobs():
    """
    ì±„ìš© ê³µê³  íŒŒì‹± (Playwright ì‚¬ìš©)
    """
    base_url = "https://enki.career.greetinghr.com"
    list_url = f"{base_url}/ko/guide?employments=INTERN_WORKER"
    
    jobs = []

    try:
        with sync_playwright() as p:
            # ë¸Œë¼ìš°ì € ì‹¤í–‰
            browser = p.chromium.launch(headless=True)
            context = browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            )
            page = context.new_page()

            logger.info(f"Accessing list page: {list_url}")
            page.goto(list_url)
            
            # ê³µê³  ëª©ë¡ì´ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸° (ìµœëŒ€ 10ì´ˆ)
            try:
                page.wait_for_selector("a[href^='/ko/o/']", timeout=10000)
            except:
                logger.warning("Timeout waiting for job list. Page might be empty.")
                return []

            # ëª©ë¡ í˜ì´ì§€ íŒŒì‹±
            html = page.content()
            soup = BeautifulSoup(html, 'html.parser')
            job_links = soup.select("a[href^='/ko/o/']")
            
            logger.info(f"Found {len(job_links)} job postings.")

            for link_tag in job_links:
                try:
                    relative_link = link_tag['href']
                    if relative_link.endswith('/apply'):
                        continue
                        
                    detail_url = f"{base_url}{relative_link}"
                    job_id = relative_link.split("/")[-1]

                    logger.info(f"Scraping job: {detail_url}")
                    
                    # ìƒì„¸ í˜ì´ì§€ ì´ë™
                    page.goto(detail_url)
                    
                    # í•µì‹¬ ë‚´ìš©(ì§êµ° ë“±)ì´ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
                    try:
                        page.wait_for_selector("span", timeout=5000)
                        time.sleep(1) # ë Œë”ë§ ì•ˆì •í™” ëŒ€ê¸°
                    except:
                        pass

                    detail_html = page.content()
                    detail_soup = BeautifulSoup(detail_html, 'html.parser')
                    
                    # ì œëª© ì¶”ì¶œ
                    title = "Unknown Job"
                    og_title = detail_soup.find("meta", property="og:title")
                    if og_title:
                        title = og_title["content"]
                    else:
                        h1 = detail_soup.find("h1")
                        if h1:
                            title = h1.text.strip()

                    # í•„ë“œ ì¶”ì¶œ
                    fields = {
                        "ì§êµ°": "",
                        "ì§ë¬´": "",
                        "ê²½ë ¥ì‚¬í•­": "",
                        "ê³ ìš©í˜•íƒœ": "",
                        "ë§ˆê°ê¸°í•œ": ""
                    }
                    
                    spans = detail_soup.find_all("span")
                    for span in spans:
                        text = span.text.strip()
                        if text in fields:
                            try:
                                # 1. Label Span -> Parent Div -> Next Sibling Span
                                label_div = span.parent
                                value_span = label_div.find_next_sibling("span")
                                
                                if value_span:
                                    fields[text] = value_span.text.strip()
                            except:
                                pass

                    job_data = {
                        "_id": job_id,
                        "title": title,
                        "url": detail_url,
                        "group": fields["ì§êµ°"],
                        "duty": fields["ì§ë¬´"],
                        "experience": fields["ê²½ë ¥ì‚¬í•­"],
                        "type": fields["ê³ ìš©í˜•íƒœ"],
                        "deadline": fields["ë§ˆê°ê¸°í•œ"],
                        "scraped_at": datetime.datetime.now()
                    }
                    
                    jobs.append(job_data)
                    time.sleep(1)

                except Exception as e:
                    logger.error(f"Error parsing job: {e}")
                    continue
            
            browser.close()

    except Exception as e:
        logger.error(f"Playwright error: {e}")
        return []
            
    return jobs

def send_discord_webhook(job, webhook_url):
    """
    ë””ìŠ¤ì½”ë“œ ì•Œë¦¼ ì „ì†¡
    """
    embed = {
        "title": f"ğŸ“¢ New Job Opening: {job['title']}",
        "url": job['url'],
        "color": 0x00ff00, # Green
        "fields": [
            {"name": "ì§êµ°", "value": job['group'], "inline": True},
            {"name": "ì§ë¬´", "value": job['duty'], "inline": True},
            {"name": "ê³ ìš©í˜•íƒœ", "value": job['type'], "inline": True},
            {"name": "ë§ˆê°ê¸°í•œ", "value": job['deadline'], "inline": True},
            {"name": "ê²½ë ¥ì‚¬í•­", "value": job['experience'], "inline": False},
        ],
        "footer": {"text": f"Enki Job Notification â€¢ {datetime.datetime.now().strftime('%Y-%m-%d %H:%M')}"}
    }

    payload = {
        "username": "Enki Bot",
        "embeds": [embed]
    }

    try:
        requests.post(webhook_url, json=payload)
        logger.info(f"Sent webhook for {job['title']}")
    except Exception as e:
        logger.error(f"Webhook error: {e}")

def sync_and_notify(current_jobs, mongo_uri, webhook_url):
    """
    DB ë™ê¸°í™” ë° ì•Œë¦¼
    """
    try:
        client = pymongo.MongoClient(mongo_uri)
        db = client.get_database('webhook')
        collection = db['Job_Latest']
        status_collection = db['Job_Status']

        stored_ids = [doc['_id'] for doc in collection.find({}, {"_id": 1})]
        current_ids = [j['_id'] for j in current_jobs]

        new_jobs = [j for j in current_jobs if j['_id'] not in stored_ids]
        removed_ids = set(stored_ids) - set(current_ids)

        logger.info(f"Sync: {len(new_jobs)} new, {len(removed_ids)} removed")

        for job in new_jobs:
            collection.insert_one(job)
            send_discord_webhook(job, webhook_url)
            time.sleep(1)

        if removed_ids:
            collection.delete_many({"_id": {"$in": list(removed_ids)}})

        # ì‹¤í–‰ ì‹œê°„ ì—…ë°ì´íŠ¸
        status_collection.update_one(
            {"_id": "scraper_status"},
            {"$set": {"last_run": datetime.datetime.now()}},
            upsert=True
        )

    except Exception as e:
        logger.error(f"DB Error: {e}")
    finally:
        client.close()

def check_last_run(mongo_uri):
    """
    í•˜ë£¨ 1íšŒ ì‹¤í–‰ ì²´í¬
    """
    try:
        client = pymongo.MongoClient(mongo_uri)
        db = client.get_database('webhook')
        status_collection = db['Job_Status']
        
        doc = status_collection.find_one({"_id": "scraper_status"})
        if not doc or not doc.get("last_run"):
            return False
            
        if doc["last_run"].date() == datetime.datetime.now().date():
            return True
            
        return False
    except:
        return False

if __name__ == "__main__":
    logger.info("Starting Job Scraper")
    env = get_env_var()
    if env:
        if check_last_run(env["MONGO_URI"]):
            logger.info("Already ran today. Exiting.")
            exit(0)
            
        jobs = parse_jobs()
        if jobs:
            sync_and_notify(jobs, env["MONGO_URI"], env["DISCORD_WEBHOOK"])
        else:
            logger.info("No jobs found.")
    logger.info("Finished.")
